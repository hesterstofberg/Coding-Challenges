{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on MBTI\n",
    "© Explore Data Science Academy\n",
    "\n",
    "## Learning Objectives\n",
    "In this train you will:\n",
    "- Learn how to apply machine learning models (i.e. logistic regression) to text data;\n",
    "- Learn how to deal with class imbalance;\n",
    "- Understand multi-class classification; and \n",
    "- Understand how to assess model performance in the multi-class classification case.\n",
    "\n",
    "## Outline\n",
    "This train is structured into the following sections:\n",
    "- Dealing with Class imbalance\n",
    "- Multi-class Classification with Logistic Regression\n",
    "- Logistic Regression on MBTI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Class Imbalance\n",
    "\n",
    "In previous trains we've mentioned a phenomenon known as class imbalance. This occurs when the number of observations across different class labels are unevenly distributed. In training our classification model, it is preferable for all classes to have a relatively even split of observations. However, in the wild, classification datasets often come with unevenly distributed observations with one class or set of classes having way more observations than others.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/class-imbalance.png?raw=true\" width=80% alt=\"class imbalance img\">\n",
    "\n",
    "### Resampling\n",
    "\n",
    "As it turns out, some clever scientists have come up with various ways to address this so-called class imbalance problem. Here we will discuss two variants of the most common method available: **resampling**. Put simply, resampling methods involve modifying the number of observations in each class as follows:\n",
    "\n",
    "- **Downsampling** - taking a random subset of the majority class small enough to match the number of observations in the minority class.\n",
    "\n",
    "- **Upsampling** - taking repeated random samples from the minority class until we have as many observations as the  majority class. This grows the size of the minority class by effectively duplicating observations at random.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/upsample-downsample.png?raw=true\" width=80% alt=\"class imbalance img\">\n",
    "\n",
    "Let's use an example to demonstrate how these work.\n",
    "\n",
    "### Resampling the Email Spam Classification dataset\n",
    "\n",
    "One famous dataset for email spam detection is the [Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/spambase), which contains a set of features indicating whether or not a particular email is spam. We choose this dataset because of its inherent class imbalance. As you can imagine, the average person receives way more non-spam email compared to spam email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load a modified version of the Spambase dataset\n",
    "df = pd.read_csv('https://github.com/Explore-AI/Public-Data/blob/master/Data/classification_sprint/unbalanced_email_spam_data.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.695</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.315</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.051</td>\n",
       "      <td>13.820</td>\n",
       "      <td>104</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.260</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.444</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>1.890</td>\n",
       "      <td>18</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.417</td>\n",
       "      <td>49</td>\n",
       "      <td>270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00                0.0           1.63           0.0   \n",
       "1            0.00                0.0           0.00           0.0   \n",
       "2            0.00                0.0           0.00           0.0   \n",
       "3            0.25                0.0           0.25           0.0   \n",
       "4            0.25                0.5           0.50           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.00             0.0              0.00                 0.0   \n",
       "1           0.26             0.0              0.26                 0.0   \n",
       "2           0.00             0.0              0.00                 0.0   \n",
       "3           0.50             0.0              0.25                 0.0   \n",
       "4           0.00             0.0              0.00                 0.0   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0              0.0            0.00  ...        0.000        0.000   \n",
       "1              0.0            0.00  ...        0.462        0.084   \n",
       "2              0.0            0.00  ...        0.000        0.000   \n",
       "3              0.0            0.00  ...        0.000        0.041   \n",
       "4              0.0            0.25  ...        0.000        0.181   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0        0.000        2.695        0.000        0.000   \n",
       "1        0.084        0.378        0.000        1.051   \n",
       "2        0.000        0.000        3.260        0.000   \n",
       "3        0.000        0.082        0.041        0.041   \n",
       "4        0.000        0.407        0.997        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       2.315                          12   \n",
       "1                      13.820                         104   \n",
       "2                       2.444                          10   \n",
       "3                       1.890                          18   \n",
       "4                       3.417                          49   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                        44     1  \n",
       "1                      1078     1  \n",
       "2                        44     1  \n",
       "3                       225     1  \n",
       "4                       270     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick analysis of the distribution of observations across our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate minority and majority classes\n",
    "not_spam = df[df['spam']==0]\n",
    "spam = df[df['spam']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all possible labels\n",
    "labels = df['spam'].unique()\n",
    "heights = [len(spam),len(not_spam)]\n",
    "plt.bar(labels,heights,color='grey')\n",
    "plt.xticks(labels,['spam','not spam'])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709778194314277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of non spam emails in the dataset \n",
    "len(not_spam)/(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our two classes are imbalanced. To put this in perspective as to why this is such an issue, say we had a model that always predicts that a given email is not spam (clearly a really bad model!). Such a model would achieve an accuracy of 87%! This is why (as explained in previous trains), class imbalance can be a serious problem if left unchecked. \n",
    "\n",
    "Let's use resampling techniques to fix this. Notice that we are keeping our features and labels together for the time being so that they get sampled together (otherwise we risk mixing labels and observations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we start by importing our modules\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Downsampling the majority class\n",
    "\n",
    "Since the `not_spam` class has so many observations, we can reduce it's size by taking a small random subset of observations to match the size of the `spam` class. Because this approach reduces the overall size of the dataset, it makes sense to use it only in cases where we have a big collection of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    413\n",
       "0    413\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample majority\n",
    "not_spam_downsampled = resample(not_spam,\n",
    "                          replace=False, # sample without replacement (no need to duplicate observations)\n",
    "                          n_samples=len(spam), # match number in minority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "downsampled = pd.concat([not_spam_downsampled, spam])\n",
    "\n",
    "# Check new class counts\n",
    "downsampled['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "downsampled_heights = [len(downsampled[downsampled['spam']==0]),len(downsampled[downsampled['spam']==1])]\n",
    "\n",
    "# Get all possible labels\n",
    "labels = df['spam'].unique()\n",
    "plt.bar(labels,heights,color='grey')\n",
    "plt.bar(labels,downsampled_heights,color='orange')\n",
    "plt.xticks(labels,['spam','not spam'])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.legend(['original','resampled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Upsampling the minority class\n",
    "\n",
    "Here, we simply make random copies of observations in the minority class until we match the size of the majority class. Using this approach means we end up with more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2788\n",
       "0    2788\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsample minority\n",
    "spam_upsampled = resample(spam,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=len(not_spam), # match number in minority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "upsampled = pd.concat([spam_upsampled, not_spam])\n",
    "\n",
    "# Check new class counts\n",
    "upsampled['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAblklEQVR4nO3de7iUZb3G8e/NQVFBUUBDQEHDUnOLuFIMjxkInqjMFBLRNNyIpR0ssnbgaWdlmJZpmCgKQponVBSR8FgooCgiKYSES9iIaCgS59/+Y55FI6y13gGZNQPr/lzXXDPzvKffrGvBvZ73feZ5FRGYmZnVpkGpCzAzs/LnsDAzs0wOCzMzy+SwMDOzTA4LMzPL1KjUBRRDy5Yto3379qUuw8xsqzJt2rR3I6JVdcu2ybBo3749U6dOLXUZZmZbFUn/rGmZT0OZmVkmh4WZmWVyWJiZWaZt8pqFmW3dVq9eTWVlJStWrCh1KdukJk2a0LZtWxo3blzwNg4LMys7lZWVNGvWjPbt2yOp1OVsUyKCJUuWUFlZSYcOHQrezqehzKzsrFixghYtWjgoikASLVq02ORem8PCzMqSg6J4Nudn67AwM7NMvmZhZuXvri3cy+izdd7HZ8iQITRt2pQf/OAHBW/TtGlTli1b9omP7bCoxuWXX17qEqxMDR48uNQl5Gzp/zzLzX6PwpKPirf/JZs2w0NEEBE0aFDikzHLF4B2qL3+FhVFObTDwsysGvPmL6DnGRdz3JGH8repM7jkgt7cfPt9rFy5in07tOW2G35G06Y7MuiK3zL2sWdo1Kgh3Y89nGuvuISHHnuaq4YOZ9Xq1bTYdRdG3Xwle+zegiG/GMab8xewcNG7vPGP+Qy98hImT32VRyf+lTatW/HQqOto3LgR7Q85lTO+3I1Jz+ZC4a4/XMWn92n3sfr+8WYlA3/0Cxa/+y923LEJt1z3Ez7bsT1vvvkmffr0Yc2aNfTo0WOL/Tx8zcLMrAavz/knZ59xEhP+fCO3jhzLE/feyIuTRlLRaX+G3jSK995fyv2PPMnM5/7EK0+P5qffPw+AI7t0YvL423hp0ijO/Ep3fvnbO9bv8x/zKnlk9G948M5rOWvAzzjuyEOZ8cwYdmjShEcmPLt+vZ2b7cQLE0Zw0flf55KfDN2otv7fu5rf/vxSpv3lTq69/GIuvPQXAFx88cUMGDCAKVOm8KlPfWqL/SzcszAzq8He7VrTpeIgHh7/DK+9MZeuJ+XCYNWqNRzx+YPYudlONGmyPedfchUndevKyd2PAqBywTuccf5lLFz0LqtWrabD3nuu32fP479A48aNOOiAT7N27Tp6HP8FAA7af1/mzV+wfr3eX+2enk/guz+97mN1LVu2nL9OmcHp5w1a37Zy5WoAnnvuOe69914A+vbty49+9KMt8rNwWJiZ1WCnHZsAuWsW3Y45nNG3XL3ROi88fjsTn57CmPsf53d/vIe/PHAT3x70K743oA+n9jyGJ5+dxpBfDlu//vbb5b413aBBAxo3brR+GGuDBg1Ys2bt+vXyh7duONJ1Xayj+c5Nmf7kXdXWXYxhxz4NZWaWoUvFQTz3wsvMmfsWAMuXr+CNOf9k2bLlLP1gGSd268pvrv4e0199A4ClHy6jTevdARjxp4c365h/emBC7vn+xzmi4qCPLdu5WVM67L0n9zz4BJALs5fTsbt27cqYMWMAGDVq1GYduzruWZhZ+TthSkkP36rlrtz+28H07v8TVq7Kne656sf/TbOmO9Gr7/dZsXIVEcF1V30XgCGX9uf08wbRpvXudDn0c7z5zwW17b5aK1eu5vDu57Bu3TpGD9u4RzPq5isZcOk1XDV0OKtXr+HMr3Tj4M/tx/XXX0+fPn24/vrrOe200z7ZB8+jiK1zvHFtKioq4pPc/MhDZ60mHjpbN2bt9yj7d2hZ6jJKpv0hpzL1iTto2aL5pm9c4NDZWbNmsf/++3+sTdK0iKh2Bz4NZWZmmXwaysyszMx7aWypS9iIexZmZpbJYWFmZpkcFmZmlslhYWZmmXyB28zK3uW/e2SL7m/wRSdtsX2deObF3PWHq2i+S7Ma1/nZz2/m6C8cwpeOOXyT9//ks9O49saRPDz6uuyVi8hhYWa2GaqmLR835vrMda/48X/XQUXF5dNQZmY1GPr7UXzuyDP43JFn8Jub72Le/AXsf8TpXHjpNXT+4lm89fYi2h9yKu8u+RcAV177Rz7b5Wt0O20gvb/1E6793Z0AnHPREP48diKQ+8Ld4Gv+QOfjzuKgo87k77PnAfDCizP5Qs9vcshx3+ALPb/J66m9XDgszMyqMW36LG4b/RDPj7+dyY/dxi13PsD7//pw/bTlL00axd7tWq9ff+pLr3Hvw3/hpUkjue/2XzJ1+qwa992yRXNenDSSAeeexrW/GwnAZzvuzdMPDeOlSaO4YtAFXHb174v+GTdF0U5DSWoH3AF8ClgHDIuI6yUNAb4FLE6rXhYR49I2PwbOA9YC34mI8am9B3A90BD4Y0RcU6y6zcwAnn1+Ol858Vh22mkHAL568nE8M/ml9dOWV7d+rx7HsMMOuZlqTznhqBr3/dWTjwPg0IP3576HJwGw9INl9Bt4ObPnzkcSq1ev2dIf6RMp5jWLNcD3I+JFSc2AaZImpGXXRcS1+StLOgA4EzgQ2BN4QtJ+afGNQDegEpgiaWxEvFbE2s2snqtp3ryqacs3Xr/wfW+/3XYANMyblvx/fn4zxx15KPff8SvmzV/Asb3K6zpH0U5DRcTCiHgxvf4QmAW0qWWTXsCYiFgZEW8Cc4DD0mNORMyNiFXAmLSumVnRHH1EZx549CmWL1/BRx/9m/sfeZKjuhxS4/pHHn4wD41/hhUrVrJs2fKP3fWuEEs/+Gj9tOa3j968ac2LqU5GQ0lqDxwCPA90BS6SdDYwlVzv431yQTI5b7NK/hMub23QvtH4M0n9gf4Ae+2115b9AGZWUltyqGuhOh/8Wc4582QO694PgPPP6sWuzWseHvv5zgdyao+jOfiYPuzdrjUVnfZnl52bFny8H367L/0GXs7Qm0bxxaMKmzm2LhV9inJJTYGngKsj4j5JewDvAgFcCbSOiG9KuhH4W0SMTNvdCowj1/s5ISLOT+19gcMi4ts1HdNTlFuxeIryurG1TlG+bNlymjbdkeXLV3D0Kf0ZNvQyOh/82botokhTlBe1ZyGpMXAvMCoi7gOIiEV5y28BqvpblUC7vM3bAlV3DKmp3cysbPT/3v/y2htzWbFiFf3OPKnug6KIijkaSsCtwKyIGJrX3joiFqa3XwFeTa/HAndJGkruAndH4AVAQEdJHYC3yV0E71Osus3MNtddw64qdQlFU8yeRVegLzBD0vTUdhnQW1Incqeh5gEXAETETEl3A6+RG0k1MCLWAki6CBhPbujs8IiYWcS6zazUYh0RoG37bFvJbM7lh6KFRUQ8S65XsKFxtWxzNbDRzWbT9zBq3M7Mti1NVs5hyYe70aJZIwfGFhYRLFmyhCZNqh8CXBPPDWVmZaft20OoZAiLt/80yBNNbJJ3av7meJUmTZrQtm3bTdqtw8LMyk7jte/TYf7FpS5j69SnOCNcHdlmZpbJYWFmZpkcFmZmlslhYWZmmRwWZmaWyWFhZmaZHBZmZpbJYWFmZpkcFmZmlslhYWZmmRwWZmaWyWFhZmaZHBZmZpbJYWFmZpkcFmZmlslhYWZmmRwWZmaWyWFhZmaZHBZmZpbJYWFmZpkcFmZmlslhYWZmmRwWZmaWyWFhZmaZHBZmZpbJYWFmZpmKFhaS2kmaJGmWpJmSLk7tu0maIGl2et41tUvSDZLmSHpFUue8ffVL68+W1K9YNZuZWfUyw0LSxZJ2Tv+Z3yrpRUndC9j3GuD7EbE/0AUYKOkAYBAwMSI6AhPTe4CeQMf06A/clI6/GzAYOBw4DBhcFTBmZlY3CulZfDMiPgC6A62Ac4FrsjaKiIUR8WJ6/SEwC2gD9AJGpNVGAF9Or3sBd0TOZKC5pNbACcCEiHgvIt4HJgA9Cv2AZmb2yRUSFkrPJwK3RcTLeW0FkdQeOAR4HtgjIhZCLlCA3dNqbYC38jarTG01tW94jP6Spkqaunjx4k0pz8zMMhQSFtMkPU4uLMZLagasK/QAkpoC9wKXpB5KjatW0xa1tH+8IWJYRFREREWrVq0KLc/MzApQSFicR+66wucjYjmwHblTUZkkNSYXFKMi4r7UvCidXiI9v5PaK4F2eZu3BRbU0m5mZnUkMywiYh2wCDhA0tHAgUDzrO0kCbgVmBURQ/MWjQWqRjT1Ax7Maz87XUjvAixNp6nGA90l7ZoubHdPbWZmVkcaZa0g6RfAGcBrwNrUHMDTGZt2BfoCMyRNT22Xkbs4frek84D5wOlp2Thyp7rmAMtJvZeIeE/SlcCUtN4VEfFe9kczM7MtJTMsyI1W+kxErNyUHUfEs9R8Ifz4atYPYGAN+xoODN+U45uZ2ZZTyDWLuUDjYhdiZmblq5CexXJguqSJwPreRUR8p2hVmZlZWSkkLMamh5mZ1VOZYRERIyRtB+yXml6PiNXFLcvMzMpJIaOhjiU3Lcc8ches20nqFxFZo6HMzGwbUchpqF8D3SPidQBJ+wGjgUOLWZiZmZWPQkZDNa4KCoCIeAOPjjIzq1cK6VlMlXQrcGd6/w1gWvFKMjOzclNIWAwg92W575C7ZvE08PtiFmVmZuWlkNFQK4Gh6WFmZvVQjWEh6e6I+LqkGVQ/Jfh/FbUyMzMrG7X1LC5OzyfXRSFmZla+ahwNVXU3O+DCiPhn/gO4sG7KMzOzclDI0Nlu1bT13NKFmJlZ+artmsUAcj2IfSS9kreoGfBcsQszM7PyUds1i7uAR4Gfk7utapUPffMhM7P6pcawiIilwFKgN4Ck3YEmQFNJTSNift2UaGZmpZZ5zULSKZJmA28CT5GbUPDRItdlZmZlpJAL3FcBXYA3IqIDuVui+pqFmVk9UkhYrI6IJUADSQ0iYhLQqch1mZlZGSlkbqh/SWpKbk6oUZLeAdYUtywzMysnhfQsepG7D/d3gceAfwCnFLMoMzMrL4X0LPoD90REJbk75pmZWT1TSM9iZ2C8pGckDZS0R7GLMjOz8pIZFhFxeUQcSO6eFnsCT0l6ouiVmZlZ2SikZ1HlHeD/gCXA7sUpx8zMylEhX8obIOlJYCLQEviW72VhZla/FHKBey/gkoiYXuxizMysPNXas5DUADhlc4JC0nBJ70h6Na9tiKS3JU1PjxPzlv1Y0hxJr0s6Ia+9R2qbI2nQhscxM7PiqzUsImId8LKkvTZj37cDPappvy4iOqXHOABJBwBnAgembX4vqaGkhsCN5O6fcQDQO61rZmZ1qJDTUK2BmZJeAD6qaoyIU2vbKCKeltS+wDp6AWMiYiXwpqQ5wGFp2ZyImAsgaUxa97UC92tmZltAIWFx+RY+5kWSzgamAt+PiPeBNsDkvHUqUxvAWxu0H17dTiX1J/cFQvbaa3M6QmZmVpNCvmdRNS154/R6CvDiZh7vJmBfchMRLgR+ndpV3aFraa+uzmERURERFa1atdrM8szMrDqFDJ39FvBn4A+pqQ3wwOYcLCIWRcTadC3kFv5zqqkSaJe3altgQS3tZmZWhwr5Ut5AoCvwAUBEzGYzv5QnqXXe268AVSOlxgJnStpeUgegI/ACuV5MR0kdJG1H7iL42M05tpmZbb5CrlmsjIhVUu6MkKRG1HAqKJ+k0cCxQEtJlcBg4FhJndL284ALACJipqS7yV24XgMMjIi1aT8XAeOBhsDwiJi5KR/QzMw+uULC4ilJlwE7SOoGXAg8lLVRRPSupvnWWta/Gri6mvZxwLgC6jQzsyIp5DTUIGAxMINcT2Ac8NNiFmVmZuUls2eRdzH6Fkm7AW0jIvM0lJmZbTsKGQ31pKSdU1BMB26TNLT4pZmZWbko5DTULhHxAfBV4LaIOBT4UnHLMjOzclJIWDRKQ16/Djxc5HrMzKwMFRIWV5AbuvqPiJgiaR9gdnHLMjOzclLIBe57gHvy3s8FTitmUWZmVl4KucC9j6SHJC1O96d4MH3L2szM6olCTkPdBdxNbqryPcn1MsYUsygzMysvhYSFIuLOiFiTHiMpYLoPMzPbdtR4zSJ9rwJgUrqd6RhyIXEG8Egd1GZmZmWitgvc0/j4PSUuyFsWwJXFKsrMzMpLjWEREb6IbWZmQAFDZyU1BgYAR6emJ4E/RMTqItZlZmZlpJApym8CGgO/T+/7prbzi1WUmZmVl0LC4vMRcXDe+79IerlYBZmZWfkpZOjsWkn7Vr1J032sLV5JZmZWbgrpWVxKbvjsXHIjo/YGzi1qVWZmVlYKmRtqoqSOwGfIhcXfI2Jl0SszM7OyUUjPghQOrxS5FjMzK1OFXLMwM7N6rsawkNQ1PW9fd+WYmVk5qq1ncUN6/ltdFGJmZuWrtmsWqyXdBrSRdMOGCyPiO8Ury8zMykltYXEy8CXgi+QmFTQzs3qqtokE3wXGSJoVEf7GtplZPVbIaKglku5Pt1RdJOleSW2LXpmZmZWNQsLiNmAsuVuqtgEeSm1mZlZPFBIWu0fEbXm3Vb0daJW1kaThqTfyal7bbpImSJqdnndN7ZJ0g6Q5kl6R1Dlvm35p/dmS+m3GZzQzs0+okLBYLOksSQ3T4yxgSQHb3Q702KBtEDAxIjoCE9N7gJ5Ax/ToT24K9Kpbuw4GDgcOAwZXBYyZmdWdQsLim8DXgf8DFgJfS221ioingfc2aO4FjEivRwBfzmu/I3ImA80ltQZOACZExHsR8T4wgY0DyMzMiqyQiQTnA6duoePtEREL034XSto9tbcB3spbrzK11dRuZmZ1qFzmhlI1bVFL+8Y7kPpLmipp6uLFi7docWZm9V1dh8WidHqJ9PxOaq8E2uWt1xZYUEv7RiJiWERURERFq1aZ19/NzGwT1HVYjAWqRjT1Ax7Maz87jYrqAixNp6vGA90l7ZoubHdPbWZmVocyw0LST/NeFzwDraTR5CYh/IykSknnAdcA3STNBrql9wDjgLnAHOAW4EKAiHgPuBKYkh5XpDYzM6tDNV7glvRD4Blyo5+uSs1/AzrXtE2+iOhdw6Ljq1k3gIE17Gc4MLyQY5qZWXHUNhrqdeB0YB9JzwCzgBaSPhMRr9dJdWZmVhZqOw31PnAZuVNDx/Kf+1sMkvTXItdlZmZlpLaeRQ9y357eFxgKvAx8FBHn1kVhZmZWPmrsWUTEZRFxPDAPGEkuWFpJelbSQ3VUn5mZlYHMb3AD4yNiCjBF0oCIOFJSy2IXZmZm5SNz6GxE/DDv7Tmp7d1iFWRmZuVnk76U5zvmmZnVT+UyN5SZmZUxh4WZmWVyWJiZWSaHhZmZZXJYmJlZJoeFmZllcliYmVkmh4WZmWVyWJiZWSaHhZmZZXJYmJlZJoeFmZllcliYmVkmh4WZmWVyWJiZWSaHhZmZZXJYmJlZJoeFmZllcliYmVkmh4WZmWVyWJiZWSaHhZmZZSpJWEiaJ2mGpOmSpqa23SRNkDQ7Pe+a2iXpBklzJL0iqXMpajYzq89K2bM4LiI6RURFej8ImBgRHYGJ6T1AT6BjevQHbqrzSs3M6rlyOg3VCxiRXo8AvpzXfkfkTAaaS2pdigLNzOqrUoVFAI9Lmiapf2rbIyIWAqTn3VN7G+CtvG0rU9vHSOovaaqkqYsXLy5i6WZm9U+jEh23a0QskLQ7MEHS32tZV9W0xUYNEcOAYQAVFRUbLTczs81Xkp5FRCxIz+8A9wOHAYuqTi+l53fS6pVAu7zN2wIL6q5aMzOr87CQtJOkZlWvge7Aq8BYoF9arR/wYHo9Fjg7jYrqAiytOl1lZmZ1oxSnofYA7pdUdfy7IuIxSVOAuyWdB8wHTk/rjwNOBOYAy4Fz675kM7P6rc7DIiLmAgdX074EOL6a9gAG1kFpZmZWg1Jd4DazT+Dy2UNKXYKVqcFF2m85fc/CzMzKlMPCzMwyOSzMzCyTw8LMzDI5LMzMLJPDwszMMjkszMwsk8PCzMwyOSzMzCyTw8LMzDI5LMzMLJPDwszMMjkszMwsk8PCzMwyOSzMzCyTw8LMzDI5LMzMLJPDwszMMjkszMwsk8PCzMwyOSzMzCyTw8LMzDI5LMzMLJPDwszMMjkszMwsk8PCzMwyOSzMzCzTVhMWknpIel3SHEmDSl2PmVl9slWEhaSGwI1AT+AAoLekA0pblZlZ/bFVhAVwGDAnIuZGxCpgDNCrxDWZmdUbiohS15BJ0teAHhFxfnrfFzg8Ii7KW6c/0D+9/Qzwep0Xum1qCbxb6iLMauHf0S1n74hoVd2CRnVdyWZSNW0fS7mIGAYMq5ty6g9JUyOiotR1mNXEv6N1Y2s5DVUJtMt73xZYUKJazMzqna0lLKYAHSV1kLQdcCYwtsQ1mZnVG1vFaaiIWCPpImA80BAYHhEzS1xWfeFTe1bu/DtaB7aKC9xmZlZaW8tpKDMzKyGHhZmZZXJY1FOSzpG0Z6nrMLOtg8Oi/joHcFiYWUEcFtsASe0lzZJ0i6SZkh6XtENa1knSZEmvSLpf0q7pG/EVwChJ06vWzdvfdyS9lrYZk9qGSLpT0l8kzZb0rdTeVNJESS9KmiGpV15Nf5f0R0mvShol6UuSnkvbH1a3PyXbGkjaSdIjkl5OvzdnSJon6ReSXkiPT6d1T5H0vKSXJD0haY/UPkTSiPTvYJ6kr0r6Zfr9fExS49J+yq1URPixlT+A9sAaoFN6fzdwVnr9CnBMen0F8Jv0+kmgoob9LQC2T6+bp+chwMvADuSmV3iLXM+kEbBzWqclMIfcN+6rajqI3B8l04DhaVkv4IFS/9z8KL8HcBpwS977XYB5wE/S+7OBh9PrXfnPiM7zgV+n10OAZ4HGwMHAcqBnWnY/8OVSf86t8eGexbbjzYiYnl5PA9pL2oXcf/ZPpfYRwNEF7OsVcr2Os8j9h1/lwYj4d0S8C0wiN8GjgP+V9ArwBNAG2COvphkRsQ6YCUyM3L/YGeTCxGxDM4AvpZ7EURGxNLWPzns+Ir1uC4yXNAO4FDgwbz+PRsTqtL+GwGN5+29fxPq3WQ6LbcfKvNdr+WRfuDyJ3JTwhwLTJFXta8Mv5QTwDaAVcGhEdAIWAU2qqWld3vt1n7A+20ZFxBvkfu9mAD+X9LOqRfmrpeffAr+LiIOAC/jP7x2k37X0h8rq9EcK+HdvszkstmHpr7L3JR2VmvoCVb2MD4FmG24jqQHQLiImAT8EmgNN0+JekppIagEcS24all2AdyJitaTjgL2L9Xls25dG6C2PiJHAtUDntOiMvOe/pde7AG+n1/3qrMh6ygm77esH3CxpR2AucG5qvz21/xs4IiL+ndobAiPTKSwB10XEvyQBvAA8AuwFXBkRCySNAh6SNBWYDvy9jj6XbZsOAn4laR2wGhgA/BnYXtLz5P7A7Z3WHQLcI+ltYDLQoe7LrT883YcVRNIQYFlEXFvqWqx+kTSP3GAM37OihHwayszMMrlnYWZmmdyzMDOzTA4LMzPL5LAwM7NMDgszM8vksDAzs0z/D+8IGgqqcV9+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "upsampled_heights = [len(upsampled[upsampled['spam']==0]),len(upsampled[upsampled['spam']==1])]\n",
    "\n",
    "# Get all possible labels\n",
    "labels = df['spam'].unique()\n",
    "plt.bar(labels,upsampled_heights,color='orange')\n",
    "plt.bar(labels,heights,color='grey')\n",
    "plt.xticks(labels,['spam','not spam'])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.legend(['resampled','original'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Best of both: upsample minority class + downsample majority class\n",
    "\n",
    "As you may have guessed, this approach involves performing both kinds or resampling techniques. \n",
    "\n",
    "#### Exercise: See if you can implement this technique by following these steps:\n",
    "\n",
    "1. Establish a **class size** (i.e. the number of observations we want in each class). For this approach to work, the **class size** has to be a value between the size of the majority class and the size of the minority class. A good heuristic to use here, is to **set the class size to be half the size of the majority class**.\n",
    "\n",
    "2. Downsample the majority class to be as small as the **class size**.\n",
    "\n",
    "3. Upsample the minority class to be as big as the **class size**.\n",
    "\n",
    "4. *Et voila!*, you should now have evenly distributed observations that you can throw at any classification model you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your awesome code here :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on the MBTI dataset\n",
    "\n",
    "Finally, let's see if we can use these resampling techinques as well as some of the concepts from previous trains to fit a logistic regression model to the mbti dataset and attempt to predict personality types given some text.\n",
    "\n",
    "As always, we start off by loading some dependencies and preprocessing our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hester/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Customise our plotting settings\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and Preprocessing the data\n",
    "\n",
    "Most of this analysis was covered in the previous tutorial. Head back there if you need a refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MBTI data\n",
    "mbti = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/mbti_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like before, split each post into multiple rows with the same label\n",
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03PMA1q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post\n",
       "0  INFJ        'http://www.youtube.com/watch?v=qsXHcwe3krw\n",
       "1  INFJ  http://41.media.tumblr.com/tumblr_lfouy03PMA1q...\n",
       "2  INFJ  enfp and intj moments  https://www.youtube.com...\n",
       "3  INFJ  What has been the most life-changing experienc...\n",
       "4  INFJ  http://www.youtube.com/watch?v=vXZeYwwRDw8   h..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lower case\n",
    "all_mbti['post'] = all_mbti['post'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out punctuation marks and numerals\n",
    "import string\n",
    "def remove_punctuation_numbers(post):\n",
    "    punc_numbers = string.punctuation + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_numbers])\n",
    "\n",
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mbti = all_mbti[['type', 'post']].groupby('type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot posts by personality types\n",
    "sum_mbti.sort_values('post', ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can see the imbalance creeping in. The introvert types have way more posts than the extroverted types. But we before we fix this using some resampling Kung Fu, let's first vectorize our data. \n",
    "\n",
    "*Can you come up with reasons why it makes sense to vectorize before resampling?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text into numbers\n",
    "\n",
    "#### 1. Features\n",
    "\n",
    "Before we can feed our data into our machine learning model, we need to first transform the text into numbers. One common method well suited for this task is Count Vectorization. We can apply this method using Sklearn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the count vectorizer with its default hyperparameters\n",
    "vect = CountVectorizer()\n",
    "X_count = vect.fit_transform(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should already be alarm bells sounding here.  We have more than 315,000 rows, which is enough data for this algorithm to run effectively.  We, however,  have almost 122,000 features - a lot of which correspond to words which only appear once (see if you can verify this on your own).  \n",
    "\n",
    "For this tutorial, we will be using the top 20 words that appear most often, to make our model easier to train. Don't worry too much about this step, we will cover hyperparameter tuning in future tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_20 = CountVectorizer(lowercase=True, stop_words='english', max_features=20,analyzer='word', ngram_range=(1, 3))\n",
    "X_count = vect_20.fit_transform(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this line to see feature names\n",
    "# vect_20.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shape of our new predictive variables\n",
    "X_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Response Variable\n",
    "\n",
    "Since our response consists of text categories, we need to somehow also convert to numerical values. Luckily for us, Sklearn has just the thing. Introducing the [Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). Unlike  `pd.get_dummies` method which creates a new column for every category it encounters, the LabelEncoder replaces each category with a number, the first category encountered will be replaced with a 0, the next one with a 1, the next with a 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit label encoder and return encoded labels\n",
    "y = le.fit_transform(all_mbti['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we have transformed our labels into a range of values between 0 and 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of label encoder types to use for lookup \n",
    "type_labels = list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the logistic regression model on standard MBTI data\n",
    "\n",
    "Finally, our data are cleaned and processed, and we are now in a position to train a logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "As mentioned in previous trains, Sklearn's logistic regression model has the capacity to accommodate multiple classes, even though logistic regression is a binary classification model. One way it does this is through a [One vs Rest](https://chrisalbon.com/machine_learning/logistic_regression/one-vs-rest_logistic_regression/) scheme (i.e. one class vs the rest of the classes). This means that we split the multi-class classification problem into multiple binary classification problems as follows:\n",
    "\n",
    "- Class 1 and not Class 1\n",
    "- Class 2 and not Class 2\n",
    "- Class 3 and not Class 3\n",
    "-   ...          ...\n",
    "- Class n and not Class n\n",
    "\n",
    "And then train a logistic regression model for each of these. At test time, we run the same test data through all the models and take the prediction of the logistic regression model with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here 'ovr' indicates that we have selected our One-vs-Rest strategy. \n",
    "logreg = LogisticRegression(multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The cell below may take a while to train depending on the number of features you have (up to 5 minutes for slower computers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking outcomes on the testing set\n",
    "\n",
    "We now investigate the performance of our newly trained models on the Test set of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_test, target_names=type_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this gives us an overall accuracy of 21% (this is the percentage of times we predict the correct class in the data). We also get a weighed F1 score of 0.07. Now besides these numbers, there are a lot of other issues here. This biggest one being that our model never predicts some of the classes. \n",
    "\n",
    "That said, the model is not completely useless since it does slightly better than random guessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the logistic regression model on balanced MBTI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to improve our model performance by rebalancing our data. Before we do so, let's first have a look at the current distribution of classes again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = [len(y[y == label]) for label in range(len(type_labels))]\n",
    "bars = pd.DataFrame(zip(heights,le.transform(type_labels).T, type_labels), columns=['heights','labels','names'])\n",
    "bars = bars.sort_values(by='heights',ascending=True)\n",
    "\n",
    "plt.bar(range(len(bars)),bars['heights'],color='grey')\n",
    "plt.xticks(range(len(bars)),bars['names'])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we have way more than two classes. This makes our resampling a bit complicated, but as before, let's decide on a **class size**. Looking at the bars above, we want to bring the minority classes up as much as possible, but at the same time we don't want to lose too much data from the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick a class size of roughly half the size of the largest size\n",
    "class_size = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to upsample anything that has samples fewer than the the class size and downsample anything with samples more than the class size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before resampling, let's stitch our features and labels together\n",
    "data = np.concatenate([X, y[:,np.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_label_df = bars.set_index('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_classes = []\n",
    "\n",
    "# For each label\n",
    "for label in range(len(type_labels)):\n",
    "    # Get num. of observations from this class\n",
    "    label_size = bar_label_df.loc[label]['heights']\n",
    "    \n",
    "    # If label_size < class size the upsample, else downsample\n",
    "    if label_size < class_size:\n",
    "        # Upsample\n",
    "        label_data = data[data[:,-1] == label]\n",
    "        label_resampled = resample(label_data,\n",
    "                                  replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                                  n_samples=class_size, # number of desired samples\n",
    "                                  random_state=27) # reproducible results\n",
    "    else:\n",
    "        # Downsample\n",
    "        label_data = data[data[:,-1] == label]\n",
    "        label_resampled = resample(label_data,\n",
    "                                  replace=False, # sample without replacement (no need for duplicate observations)\n",
    "                                  n_samples=class_size, # number of desired samples\n",
    "                                  random_state=27) # reproducible results\n",
    "        \n",
    "    resampled_classes.append(label_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data = np.concatenate(resampled_classes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split resampled data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = resampled_data[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled = resampled_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view the after image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = [len(y_resampled[y_resampled == label]) for label in range(len(type_labels))]\n",
    "bars_resampled = pd.DataFrame(zip(heights,le.transform(type_labels).T, type_labels), columns=['heights','labels','names'])\n",
    "bars_resampled = bars_resampled.sort_values(by='heights',ascending=True)\n",
    "\n",
    "plt.bar(range(len(bars)),bars['heights'],color='grey')\n",
    "plt.bar(range(len(bars_resampled)),bars_resampled['heights'],color='orange')\n",
    "plt.xticks(range(len(bars)),bars['names'])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.legend(['original','resampled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after taking a moment to appreciate what we just did. Let's keep in mind that we don't always have to resample such that all classes end up equal.\n",
    "\n",
    "### Train test split with balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the logistic regression model on our rebalanced data\n",
    "logreg = LogisticRegression(multi_class='ovr')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking outcomes on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test, y_pred_test, target_names=type_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model still has poor accuracy. But at least this time, our model is better at predicting more of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to from here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out our first model is not very good!  Don't be discouraged as our first attempt usually never is! Generally, there are a couple of methods to try and improve our model:\n",
    "* Improve the data & feature set\n",
    "* Try a different model\n",
    "* Fine tune the algorithm parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your own time, try and use the above suggestions to make this model better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Logistic Regression refresher](https://www.youtube.com/watch?v=zAULhNrnuL4)\n",
    "* [Text Classification with scikit-learn](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
